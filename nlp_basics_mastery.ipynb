{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6419b78",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be individual words or subwords, and they serve as the building blocks for further NLP tasks. Tokenization is an essential step in NLP as it helps in understanding the structure of the text and makes it easier to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbbd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries NLTK and Spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f296693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mw50000150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources (you only need to do this once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1505e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'words', 'or', 'subwords', '.']\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Tokenization is the first step in NLP. It splits text into words or subwords.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faeef9a",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy:\n",
    "spaCy is another powerful Python library for NLP that provides tokenization along with many other NLP functionalities. Install spaCy using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d33d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'words', 'or', 'subwords', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"Tokenization is the first step in NLP. It splits text into words or subwords.\"\n",
    "\n",
    "# Process the text (tokenization happens here)\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the tokens from the processed document\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d23db8",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Lowercasing: Converting all text to lowercase can help in standardizing the text and avoid discrepancies due to different cases.\n",
    "\n",
    "Removing Punctuation: Punctuation marks often do not carry significant meaning and can be removed to focus on the actual words.\n",
    "\n",
    "Tokenization: We've already covered this step, but tokenization is often considered a part of text preprocessing.\n",
    "\n",
    "Stopword Removal: Stopwords are common words like \"the,\" \"is,\" \"and,\" etc., that occur frequently in the language but typically don't contribute much to the meaning of the text. Removing them can reduce noise in the data.\n",
    "\n",
    "Stemming or Lemmatization: Reducing words to their root form can help in reducing inflectional forms and standardizing the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eea47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mw50000150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary resources (you only need to do this once)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1868227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocessing is an essential step! it helps clean and normalize the text data. preprocessing involves tokenization, stopword removal, stemming, and more.\n",
      "text preprocessing is an essential step it helps clean and normalize the text data preprocessing involves tokenization stopword removal stemming and more\n",
      "['text', 'preprocessing', 'is', 'an', 'essential', 'step', 'it', 'helps', 'clean', 'and', 'normalize', 'the', 'text', 'data', 'preprocessing', 'involves', 'tokenization', 'stopword', 'removal', 'stemming', 'and', 'more']\n",
      "['text', 'preprocessing', 'essential', 'step', 'helps', 'clean', 'normalize', 'text', 'data', 'preprocessing', 'involves', 'tokenization', 'stopword', 'removal', 'stemming']\n",
      "['text', 'preprocess', 'essenti', 'step', 'help', 'clean', 'normal', 'text', 'data', 'preprocess', 'involv', 'token', 'stopword', 'remov', 'stem']\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Text preprocessing is an essential step! It helps clean and normalize the text data. Preprocessing involves tokenization, stopword removal, stemming, and more.\"\n",
    "\n",
    "# Step 1: Convert text to lowercase\n",
    "text = text.lower()\n",
    "print(text)\n",
    "# Step 2: Remove punctuation\n",
    "import string\n",
    "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(text)\n",
    "# Step 3: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "# Step 4: Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(filtered_tokens)\n",
    "# Step 5: Stem the tokens (You can also use Lemmatization if you prefer)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Print the preprocessed tokens\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c7f9b",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging \n",
    "is a fundamental task in natural language processing that involves assigning a grammatical category (or \"part-of-speech\") to each word in a given text. The categories can include nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and more. POS tagging is useful in various NLP tasks like syntax parsing, named entity recognition, and information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83efac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mw50000150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "# Download necessary resources (you only need to do this once)\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ad8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Noordeen', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('footballer', 'NN'), (',', ','), ('especially', 'RB'), ('when', 'WRB'), ('he', 'PRP'), ('is', 'VBZ'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('midfield', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Noordeen is a great footballer, especially when he is playing in the midfield.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the part-of-speech tagged tokens\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef822b9d",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) \n",
    "is a natural language processing task that aims to identify and classify named entities in a given text. Named entities are real-world objects that have names, such as people, organizations, locations, dates, and more. NER is a crucial component of information extraction systems and is used in various applications, including information retrieval, question-answering systems, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf41c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mw50000150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mw50000150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "# Download necessary resources (you only need to do this once)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bde3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Apple\n",
      "ORGANIZATION Inc.\n",
      "PERSON Steve Jobs\n",
      "PERSON Steve Wozniak\n",
      "GPE California\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak on April 1, 1976, in California.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "\n",
    "# Print the named entities\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a1c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION Inc./NNP)\n",
      "  was/VBD\n",
      "  founded/VBN\n",
      "  by/IN\n",
      "  (PERSON Steve/NNP Jobs/NNP)\n",
      "  and/CC\n",
      "  (PERSON Steve/NNP Wozniak/NNP)\n",
      "  on/IN\n",
      "  April/NNP\n",
      "  1/CD\n",
      "  ,/,\n",
      "  1976/CD\n",
      "  ,/,\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb6db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
